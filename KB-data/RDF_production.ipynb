{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd380b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import csv\n",
    "from rdflib import *\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple, Union, Optional\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "943c6f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open files\n",
    "with open('ml_output.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d69fbf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph\n",
    "g = ConjunctiveGraph()\n",
    "\n",
    "kb = Namespace(\"http://example.org/data/\")\n",
    "g.bind(\"kb\", kb)\n",
    "\n",
    "sebi = Namespace(\"http://example.org/ontology/\")\n",
    "g.bind(\"sebi\", sebi)\n",
    "\n",
    "hico = Namespace(\"https://w3id.org/hico#\")\n",
    "g.bind(\"hico\", hico)\n",
    "\n",
    "dct = Namespace(\"http://purl.org/dc/terms/\")\n",
    "g.bind(\"dct\", dct)\n",
    "\n",
    "time = Namespace(\"https://www.w3.org/TR/owl-time/\")\n",
    "g.bind = (\"time\", time)\n",
    "\n",
    "prov = Namespace(\"http://www.w3.org/ns/prov#\")\n",
    "g.bind = (\"prov\", prov)\n",
    "\n",
    "wd = Namespace(\"http://www.wikidata.org/entity/\")\n",
    "g.bind = (\"wd\", wd)\n",
    "\n",
    "factual_data = URIRef(\"http://example.org/factual_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb361e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikidata alignment via nl label\n",
    "def wikidata_alignment_via_label(label):\n",
    "    url = \"https://www.wikidata.org/w/api.php\"\n",
    "\n",
    "    params = {\n",
    "      \"action\": \"wbsearchentities\",\n",
    "      \"format\": \"json\",\n",
    "      \"language\": \"en\",\n",
    "      \"search\": label\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    if 'search' in data and data['search']:\n",
    "        wikidata_id = data['search'][0]['id']\n",
    "        return wikidata_id\n",
    "    else:\n",
    "          return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e576b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# date semiautomatic checker\n",
    "def parse_date(date_str: str) -> Optional[Tuple[int, int]]:\n",
    "    try:\n",
    "        year = int(date_str)\n",
    "        return year, year\n",
    "    except:\n",
    "        # if months\n",
    "        months = [\n",
    "            \"January\",\n",
    "            \"February\",\n",
    "            \"March\",\n",
    "            \"April\",\n",
    "            \"May\",\n",
    "            \"June\",\n",
    "            \"July\",\n",
    "            \"August\",\n",
    "            \"September\",\n",
    "            \"October\",\n",
    "            \"November\",\n",
    "            \"December\",\n",
    "        ]\n",
    "        if any(month.lower() in date_str.lower() for month in months):\n",
    "            year_match = re.findall(r\"(\\d+)\", date_str)\n",
    "            if year_match:\n",
    "                return int(year_match[-1]), int(year_match[-1])\n",
    "\n",
    "        if \"cent\" in date_str:\n",
    "            century_match = re.match(r\"(\\d+)(st|nd|rd|th) cent\", date_str)\n",
    "            if century_match:\n",
    "                century = int(century_match.group(1))\n",
    "                return (century - 1) * 100 + 1, (century) * 100\n",
    "\n",
    "        if \"early\" in date_str:\n",
    "            century_match = re.match(r\"early (\\d+)(st|nd|rd|th) century\", date_str)\n",
    "            if century_match:\n",
    "                century = int(century_match.group(1))\n",
    "                return (century - 1) * 100 + 1, (century - 1) * 100 + 33\n",
    "            decade_match = re.match(r\"early (\\d{4})s\", date_str)\n",
    "            if decade_match:\n",
    "                decade = int(decade_match.group(1))\n",
    "                return decade, decade + 9\n",
    "\n",
    "        elif \"late\" in date_str:\n",
    "            century_match = re.match(r\"late (\\d+)(st|nd|rd|th) century\", date_str)\n",
    "            if century_match:\n",
    "                century = int(century_match.group(1))\n",
    "                return (century - 1) * 100 + 67, century * 100\n",
    "            decade_match = re.match(r\"late (\\d{4})s\", date_str)\n",
    "            if decade_match:\n",
    "                decade = int(decade_match.group(1))\n",
    "                return decade + 7, decade + 9\n",
    "\n",
    "        elif \"around\" in date_str or \"about\" in date_str:\n",
    "            year_match = re.match(r\"(around|about) (\\d+)\", date_str)\n",
    "            if year_match:\n",
    "                year = int(year_match.group(2))\n",
    "                return year - 5, year + 5\n",
    "\n",
    "        elif \"between\" in date_str:\n",
    "            between_match = re.match(r\"between (\\d+) and (\\d+)\", date_str)\n",
    "            if between_match:\n",
    "                return int(between_match.group(1)), int(between_match.group(2))\n",
    "\n",
    "        elif \"before\" in date_str:\n",
    "            before_match = re.match(r\"before (\\d+)\", date_str)\n",
    "            if before_match:\n",
    "                year = int(before_match.group(2))\n",
    "                return year - 5, year\n",
    "\n",
    "        elif \"-\" in date_str:\n",
    "            year_range_match = re.match(r\"(\\d+)[-/](\\d+)\", date_str)\n",
    "            if year_range_match:\n",
    "                return int(year_range_match.group(1)), int(year_range_match.group(2))\n",
    "\n",
    "        else:\n",
    "            beginning_input = input(\"beginning of this timespan\")\n",
    "            end_input = input(\"end of this timespan\")\n",
    "            return int(beginning_input), int(end_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8765096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_id(string):\n",
    "    # Convert to lowercase\n",
    "    id_str = string.lower()\n",
    "\n",
    "    # Remove special characters, replace spaces with dashes\n",
    "    id_str = re.sub(r'[^a-z0-9\\-]+', '-', id_str)\n",
    "\n",
    "    # Remove leading and trailing dashes\n",
    "    id_str = id_str.strip('-')\n",
    "\n",
    "    return id_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3309d3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5372c4f304de4df9a3e46100ea3d806d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "972\n",
      "837\n",
      "14th century\n",
      "March 4, 840\n",
      "14th century\n",
      "14th century\n",
      "13th century\n",
      "14th century\n",
      "8th century\n",
      "after 1704\n",
      "beginning of this timespan1704\n",
      "end of this timespan1730\n",
      "1985\n",
      "1729\n",
      "late 19th century\n",
      "shortly before 1944\n",
      "after 1935\n",
      "beginning of this timespan1935\n",
      "end of this timespan1950\n",
      "after September 1902\n",
      "1797\n",
      "since about 1950\n",
      "20th century\n",
      "1782\n",
      "around the year 1400\n",
      "20th century\n",
      "1955\n",
      "1913\n",
      "20th-century\n",
      "1913\n",
      "20th century\n",
      "1913\n",
      "1913\n",
      "1879\n",
      "between 1833 and 1842\n",
      "1834-1835\n",
      "late 14th century\n",
      "18th century\n",
      "18th century\n",
      "early 1820s\n",
      "late 4th century\n",
      "330\n",
      "reign of Julian the Apostate\n",
      "beginning of this timespan361\n",
      "end of this timespan363\n",
      "closely in the region of AD 395\n",
      "beginning of this timespan375\n",
      "end of this timespan415\n",
      "after the defeat of Magnentius\n",
      "beginning of this timespan353\n",
      "end of this timespan375\n",
      "361-380s\n",
      "Edo period\n",
      "beginning of this timespan1603\n",
      "end of this timespan1868\n",
      "early eighth century CE\n",
      "1170\n",
      "1156\n",
      "during the lifetime of Diarmaid MacMurchada\n",
      "beginning of this timespan1110\n",
      "end of this timespan1171\n",
      "1159\n",
      "1166\n",
      "around 1180\n",
      "19th-century\n"
     ]
    }
   ],
   "source": [
    "for doc in tqdm(data):\n",
    "    \n",
    "    # Document basic metadata\n",
    "    doc_id = doc[\"Page ID\"]\n",
    "    source = doc[\"Source\"]  \n",
    "    page_url = doc[\"Page URL\"]\n",
    "    doc_description = doc[\"Document description\"]\n",
    "    g.add((URIRef(f\"{kb}{doc_id}\"),dct.description,Literal(doc_description.replace(\"\\n\", \"\"), datatype=XSD.string),factual_data,))\n",
    "\n",
    "    # Claims metadata\n",
    "    for author, author_data in doc[\"Entities\"].items():\n",
    "        opinion_info = author_data[\"opinion_info\"]\n",
    "        if opinion_info.get(\"is_expressing_opinion\") == True:\n",
    "            # Opinions\n",
    "            claim_id = f\"{doc_id}-{string_to_id(author)}\"\n",
    "            claim_uri = URIRef(f\"{kb}{claim_id}\")\n",
    "            g.add((claim_uri,prov.wasQuotedFrom,Literal(page_url, datatype=XSD.anyURI),factual_data))\n",
    "            opinion = opinion_info.get(\"opinion_evaluation\", {})\n",
    "            g.add(\n",
    "                (\n",
    "                    URIRef(f\"{kb}{doc_id}\"),\n",
    "                    RDF.type,\n",
    "                    URIRef(f\"{sebi}{opinion.capitalize()}\"),\n",
    "                    claim_uri,\n",
    "                )\n",
    "            )\n",
    "            g.add((claim_uri, RDF.type, hico.InterpretationAct, factual_data))\n",
    "\n",
    "            opinion_document_author = opinion_info.get(\"opinion_document_author\", {})\n",
    "            opinion_document_date = opinion_info.get(\"opinion_document_date\", {})\n",
    "            opinion_document_location = opinion_info.get(\n",
    "                \"opinion_document_location\", {}\n",
    "            )\n",
    "\n",
    "            # Opinion dates\n",
    "            if opinion_document_date != \"null\":\n",
    "                for single_date in opinion_document_date:\n",
    "                    print(single_date)\n",
    "                    date = parse_date(single_date)\n",
    "                    if date is not None:\n",
    "                        if date[0] == date[1]:\n",
    "                            date_uri = URIRef(f\"{kb}{date[0]}\")\n",
    "                            g.add(\n",
    "                                (\n",
    "                                    date_uri,\n",
    "                                    RDFS.label,\n",
    "                                    Literal(date[0], datatype=XSD.string),\n",
    "                                    factual_data,\n",
    "                                )\n",
    "                            )\n",
    "                            g.add(\n",
    "                                (URIRef(f\"{kb}{doc_id}\"), dct.date, date_uri, claim_uri)\n",
    "                            )\n",
    "                        else:\n",
    "                            date_uri = URIRef(f\"{kb}{date[0]}-{date[1]}\")\n",
    "                            g.add(\n",
    "                                (\n",
    "                                    date_uri,\n",
    "                                    RDFS.label,\n",
    "                                    Literal(\n",
    "                                        f\"{date[0]}-{date[1]}\", datatype=XSD.string\n",
    "                                    ),\n",
    "                                    factual_data,\n",
    "                                )\n",
    "                            )\n",
    "                            g.add(\n",
    "                                (URIRef(f\"{kb}{doc_id}\"), dct.date, date_uri, claim_uri)\n",
    "                            )\n",
    "\n",
    "                        g.add(\n",
    "                            (\n",
    "                                date_uri,\n",
    "                                time.beginning,\n",
    "                                Literal(date[0], datatype=XSD.gYear),\n",
    "                                factual_data,\n",
    "                            )\n",
    "                        )\n",
    "                        g.add(\n",
    "                            (\n",
    "                                date_uri,\n",
    "                                time.end,\n",
    "                                Literal(date[1], datatype=XSD.gYear),\n",
    "                                factual_data,\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "            # Opinion authors\n",
    "            for doc_author in opinion_document_author:\n",
    "                g.add(\n",
    "                    (\n",
    "                        URIRef(f\"{kb}{doc_id}\"),\n",
    "                        dct.creator,\n",
    "                        URIRef(f\"{kb}{string_to_id(doc_author)}\"),\n",
    "                        claim_uri,\n",
    "                    )\n",
    "                )\n",
    "                g.add(\n",
    "                    (\n",
    "                        URIRef(f\"{kb}{string_to_id(doc_author)}\"),\n",
    "                        RDFS.label,\n",
    "                        Literal(doc_author, datatype=XSD.string),\n",
    "                        factual_data,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # Opinion location\n",
    "            for location in opinion_document_location:\n",
    "                wd_location = wikidata_alignment_via_label(location)\n",
    "                if wd_location != None:\n",
    "                    g.add(( URIRef(f\"{kb}{string_to_id(location)}\"), OWL.sameAs, URIRef(f'{wd}{wd_location}'), factual_data))\n",
    "                g.add((URIRef(f\"{kb}{doc_id}\"), dct.coverage, URIRef(f\"{kb}{string_to_id(location)}\"), claim_uri))\n",
    "                g.add(( URIRef(f\"{kb}{string_to_id(location)}\"), RDFS.label, Literal(location, datatype=XSD.string), factual_data))\n",
    "\n",
    "            # Evidences supporting the opinions\n",
    "            evidence_set = opinion_info.get(\"opinion_evidence_provided\", [])\n",
    "            if evidence_set != None:\n",
    "                num = 1\n",
    "                for evidence in evidence_set:\n",
    "                    evidence_uri = URIRef(f\"{kb}{claim_id}-{num}\")\n",
    "                    g.add((evidence_uri, sebi.support, claim_uri, factual_data))\n",
    "                    for evaluation, evaluation_score in evidence.items():\n",
    "                        if evaluation != \"feature\":\n",
    "                            g.add((evidence_uri, sebi.evaluate, URIRef(f\"{sebi}{evaluation}\"), factual_data))\n",
    "                            g.add((evidence_uri,sebi.hasEvaluationScore,URIRef(f\"{sebi}{evaluation_score}\"),factual_data))\n",
    "                        else:\n",
    "                            feature_URI = URIRef(f\"{sebi}{evidence['feature'].replace(' ', '_')}\")\n",
    "                            g.add((evidence_uri, sebi.assess, feature_URI, factual_data))\n",
    "                        num += 1\n",
    "\n",
    "            # Criterion\n",
    "            criterion_set = opinion_info.get(\"opinion_specific_perspective\", [])\n",
    "            if criterion_set != None:\n",
    "                for criterion in criterion_set:\n",
    "                    g.add(\n",
    "                        (\n",
    "                            claim_uri,\n",
    "                            hico.hasInterpretationCriterion,\n",
    "                            URIRef(f\"{sebi}{string_to_id(criterion)}\"),\n",
    "                            factual_data,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "            # Claim author\n",
    "            author_id = string_to_id(author)\n",
    "            author_uri = URIRef(f\"{kb}{author_id}\")\n",
    "            g.add(\n",
    "                (\n",
    "                    author_uri,\n",
    "                    RDFS.label,\n",
    "                    Literal(author, datatype=XSD.string),\n",
    "                    factual_data,\n",
    "                )\n",
    "            )\n",
    "            g.add((claim_uri, prov.wasAttributedTo, author_uri, factual_data))\n",
    "            \n",
    "            # Bibliography\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3e0170cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e25a659e1d429ba1f43d0fd3cbe8b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i=1\n",
    "with open('references.tsv', mode='r', newline='', encoding='utf-8') as file:\n",
    "    tsv_reader = csv.DictReader(file, delimiter='\\t')\n",
    "\n",
    "    for reference in tqdm(tsv_reader):\n",
    "        for doc in data:\n",
    "            doc_id = doc[\"Page ID\"]\n",
    "            if doc[\"Page ID\"] == reference['Page ID']:\n",
    "                for author, author_data in doc[\"Entities\"].items():\n",
    "                    claim_id = f\"{doc_id}-{string_to_id(author)}\"\n",
    "                    claim_uri = URIRef(f\"{kb}{claim_id}\")\n",
    "                    opinion_info = author_data[\"opinion_info\"]\n",
    "                    if opinion_info.get(\"is_expressing_opinion\") == True:\n",
    "                        info_claims = author_data.get(\"claims\", [])\n",
    "                        for info_claim in info_claims:\n",
    "                            refs = info_claim.get('bibliography', [])\n",
    "                            if refs != []:\n",
    "                                for ref in refs:\n",
    "                                    if ref in reference['Reference number']:\n",
    "                                        ref_uri = URIRef(f'{kb}ref-{i}')\n",
    "                                        g.add((claim_uri, prov.wasDerivedFrom, ref_uri, factual_data))\n",
    "                                        g.add((ref_uri, dct.description, Literal(reference['Reference'], datatype=XSD.string), factual_data))  \n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a64135c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_graph = g.serialize(destination='output_graph.trig', format='trig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
